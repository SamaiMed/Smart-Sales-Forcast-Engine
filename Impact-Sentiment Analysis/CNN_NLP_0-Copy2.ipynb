{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(800, 900) \n",
      "Test set: \t\t(200, 900)\n",
      "label set: \t\t(800,) \n",
      "Test label set: \t\t(200,)\n"
     ]
    }
   ],
   "source": [
    "with open('lab.txt', 'r') as f:\n",
    "    labels_org = f.read()\n",
    "df=pd.read_csv('Seq_Embo_f.csv',header=None)\n",
    "features=np.array(df[:][:]).astype(np.float32)\n",
    "#labels = np.array([int(_)  for _ in labels_org.split()]).astype(np.int64)\n",
    "labels =np.array([int(_)  for _ in labels_org.split()]).astype(np.float32)\n",
    "train_x,test_x,train_y,test_y=train_test_split(features,labels,test_size=0.2)\n",
    "#train_x=train_x.reshape(-1,30,30)\n",
    "#test_x=test_x.reshape(-1,30,30)\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "print(\"label set: \\t\\t{}\".format(train_y.shape), \"\\nTest label set: \\t\\t{}\".format(test_y.shape))\n",
    "\n",
    "def get_batches(x, y, batch_size=800):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len=30\n",
    "embeding_size=30\n",
    "n_input = embeding_size*seq_len\n",
    "n_classes = 1\n",
    "learning_rate = 0.01\n",
    "training_iters = 200000\n",
    "batch_size = 800\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None])\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "dropout=1\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    global embeding_size\n",
    "    global seq_len\n",
    "    x = tf.reshape(x, shape=[-1, embeding_size, seq_len, 1])\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 6])),\n",
    "    # 5x5 conv, 6 inputs, 12 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 6, 12])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([30*30*12, 150])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([150, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([6])),\n",
    "    'bc2': tf.Variable(tf.random_normal([12])),\n",
    "    'bd1': tf.Variable(tf.random_normal([150])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "pred=tf.nn.tanh(pred)\n",
    "pp=tf.reshape(pred,[-1])\n",
    "cost=tf.nn.l2_loss(y-pred)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost)\n",
    "total_error = tf.reduce_sum(tf.square(y - tf.reduce_mean(y)))\n",
    "unexplained_error = tf.reduce_sum(tf.square(y - pp))\n",
    "accuracy = 1- tf.div(total_error, unexplained_error)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 Train accuracy: 0.740497 Test accuracy: 0.778982\n",
      "1 Train accuracy: 0.72356 Test accuracy: 0.778982\n",
      "2 Train accuracy: 0.724796 Test accuracy: 0.778982\n",
      "3 Train accuracy: 0.737726 Test accuracy: 0.778982\n",
      "4 Train accuracy: 0.737167 Test accuracy: 0.778982\n",
      "5 Train accuracy: 0.737165 Test accuracy: 0.778982\n",
      "6 Train accuracy: 0.749891 Test accuracy: 0.778982\n",
      "7 Train accuracy: 0.743206 Test accuracy: 0.778982\n",
      "8 Train accuracy: 0.737165 Test accuracy: 0.778982\n",
      "9 Train accuracy: 0.738288 Test accuracy: 0.778982\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print('Initialized')\n",
    "    for epoch in range(10):\n",
    "        for io,(xx,yy) in enumerate(get_batches(train_x, train_y, 800), 1):\n",
    "            X_batch  = xx\n",
    "            y_batch  = yy\n",
    "            _,acc_train=sess.run([training_op, accuracy],feed_dict={x: X_batch, y: y_batch,keep_prob: 0.1})\n",
    "        acc_test = accuracy.eval(feed_dict={x: test_x, y: test_y, keep_prob: 1})\n",
    "        pred_test=pred.eval(feed_dict={x: test_x, y: test_y, keep_prob: 1})\n",
    "        print(epoch, \"Train accuracy:\",acc_train , \"Test accuracy:\", acc_test)\n",
    "        saver.save(sess, \"checkpoints/sento-anal_f1_CNN_enbedno_2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
