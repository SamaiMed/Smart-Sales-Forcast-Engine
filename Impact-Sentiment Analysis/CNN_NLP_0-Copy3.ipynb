{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW INT -------------->>\n",
      " [[312, 0, 5, 11, 391, 12, 20, 46, 392, 140, 65, 0, 4, 393, 24], [5, 0, 0, 313, 82, 211, 0], [5, 0, 394, 0, 92, 0, 0, 4, 314, 6], [188, 212, 213], [5, 246, 189, 395, 5, 246, 189], [0, 58, 3, 5, 0, 11, 3, 0, 0, 0, 0, 315, 0, 0, 13, 0, 11, 0, 0], [5, 190, 0, 3, 0, 30, 0, 0, 22, 0, 214], [5, 190, 4, 0, 3, 0, 214, 22, 3, 0, 9, 0], [0, 0, 113, 55, 5, 0, 0, 93], [5, 53, 0, 0, 164, 8, 165, 0, 0, 0]]\n",
      "1000\n",
      "LABELS------------->>\n",
      " [2 2 2 2 2 0 2 2 2 2]\n",
      "Zero-length reviews: 2\n",
      "Maximum review length: 30\n",
      "1000 [312, 0, 5, 11, 391, 12, 20, 46, 392, 140, 65, 0, 4, 393, 24]\n",
      "Zero-length reviews: 0\n",
      "Maximum review length: 30\n",
      "FEARTURES----------->\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 312   0   5\n",
      "  11 391  12  20  46 392 140  65   0   4 393  24]\n",
      "INT TO VOCAB----------------->>\n",
      " ['UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'Which', 'UNK', 'BMW', 'is', 'right', 'for', 'you', 'We', 'test', 'them', 'all', 'UNK', 'to', 'find', 'out']\n"
     ]
    }
   ],
   "source": [
    "with open('rev_3.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('lab.txt', 'r') as f:\n",
    "    labels_org = f.read()\n",
    "\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()\n",
    "\n",
    "vocabulary_size = 400\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    i=0\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "data, count, vocab_to_int, int_to_vocab = build_dataset(words,\n",
    "                                                            vocabulary_size)\n",
    "reviews_ints=[]\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int.get(word,0) for word in each.split()])\n",
    "print('REVIEW INT -------------->>\\n',reviews_ints[0:10])\n",
    "\n",
    "labels = np.array([int(_)  for _ in labels_org.split()]).astype(np.int64)+1\n",
    "print(len(labels))\n",
    "print('LABELS------------->>\\n',labels[:10])\n",
    "\n",
    "A=[len(x) for x in reviews_ints]\n",
    "review_lens = Counter(A)\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "\n",
    "reviews_ints = [r[0:30] for r in reviews_ints if len(r) > 0]\n",
    "print(len(reviews_ints),reviews_ints[0])\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "\n",
    "seq_len = 30\n",
    "features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "print('FEARTURES----------->\\n',features[0,:])\n",
    "\n",
    "T=[int_to_vocab.get(features[0,i],'UNK') for i in range(30)]\n",
    "print('INT TO VOCAB----------------->>\\n',T)\n",
    "del T\n",
    "\n",
    "\n",
    "def get_batches(x, y, batch_size=800):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(800, 30) \n",
      "Test set: \t\t(200, 30)\n",
      "label set: \t\t(800,) \n",
      "Test label set: \t\t(200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x,test_x,train_y,test_y=train_test_split(features,labels,test_size=0.2)\n",
    "\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))\n",
    "print(\"label set: \\t\\t{}\".format(train_y.shape), \n",
    "      \"\\nTest label set: \\t\\t{}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len=30\n",
    "embeding_size=30\n",
    "n_input = embeding_size*seq_len\n",
    "n_classes = 3\n",
    "learning_rate = 0.01\n",
    "training_iters = 200000\n",
    "batch_size = 800\n",
    "display_step = 10\n",
    "n_words = len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.int32, [None, seq_len])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "dropout=1\n",
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    global embeding_size\n",
    "    global seq_len\n",
    "    x = tf.reshape(x, shape=[-1, embeding_size, seq_len, 1])\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 6])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 6, 12])),\n",
    "    'wd1': tf.Variable(tf.random_normal([30*30*12, 150])),\n",
    "    'out': tf.Variable(tf.random_normal([150, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([6])),\n",
    "    'bc2': tf.Variable(tf.random_normal([12])),\n",
    "    'bd1': tf.Variable(tf.random_normal([150])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = tf.Variable(tf.random_uniform((n_words, embeding_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embedding, x)\n",
    "pred = conv_net(embed, weights, biases, keep_prob)\n",
    "soft_a=tf.nn.softmax(pred)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(soft_a, 1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 Train accuracy: 0.39 Test accuracy: 0.27 F1: 0.27\n",
      "1 Train accuracy: 0.24875 Test accuracy: 0.625 F1: 0.625\n",
      "2 Train accuracy: 0.63875 Test accuracy: 0.625 F1: 0.625\n",
      "3 Train accuracy: 0.63875 Test accuracy: 0.625 F1: 0.625\n",
      "4 Train accuracy: 0.63875 Test accuracy: 0.625 F1: 0.625\n",
      "5 Train accuracy: 0.63875 Test accuracy: 0.625 F1: 0.625\n",
      "6 Train accuracy: 0.64125 Test accuracy: 0.565 F1: 0.565\n",
      "7 Train accuracy: 0.61625 Test accuracy: 0.185 F1: 0.185\n",
      "8 Train accuracy: 0.21375 Test accuracy: 0.16 F1: 0.16\n",
      "9 Train accuracy: 0.21375 Test accuracy: 0.21 F1: 0.21\n"
     ]
    }
   ],
   "source": [
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print('Initialized')\n",
    "    for epoch in range(10):\n",
    "        for io,(xx,yy) in enumerate(get_batches(train_x, train_y, 800), 1):\n",
    "            X_batch  = xx\n",
    "            y_batch  = yy\n",
    "            _,acc_train=sess.run([training_op, accuracy],feed_dict={x: X_batch, y: y_batch,keep_prob: 1})\n",
    "        acc_test = accuracy.eval(feed_dict={x: test_x, y: test_y, keep_prob: 1})\n",
    "        temp=soft_a.eval(feed_dict={x: test_x, y: test_y, keep_prob: 1})\n",
    "        temp0=np.argmax(temp,axis=1)\n",
    "        temp1=sk.metrics.f1_score(test_y.reshape(-1),temp0,average='micro')\n",
    "        print(epoch, \"Train accuracy:\",acc_train , \"Test accuracy:\", acc_test,'F1:',temp1)\n",
    "        saver.save(sess, \"checkpoints/sento-anal_f1_CNN_enbedin_1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1,  1, -1,  1,  1,  0, -1,  0,  0, -1,  1, -1, -1,  1,  0,\n",
       "        0, -1, -1, -1,  0, -1,  0, -1, -1,  2,  0,  0,  1,  0,  0,  0,  0,\n",
       "        0,  1, -1,  0, -1,  0, -1, -1, -1,  1,  1, -1, -1,  0, -1, -1,  0,\n",
       "        0,  2, -1,  0,  2,  1,  0,  1, -1,  0,  1, -1,  1,  1, -1, -1,  2,\n",
       "        0,  0,  0,  1,  2,  0, -1,  0,  0,  0, -1,  0,  0, -1, -1,  0,  1,\n",
       "       -1,  0,  1, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0, -1,  0, -1,  0,\n",
       "       -1,  0, -1,  0,  0,  0,  0, -1,  0,  1, -1,  1, -1,  0,  0, -1,  0,\n",
       "        0,  0, -1,  0,  1,  0, -1,  0,  1,  0, -1,  0, -1, -1, -1,  1, -1,\n",
       "       -1,  1, -1, -1, -1,  0, -1, -1, -1, -1, -1,  0,  2, -1, -1,  0, -1,\n",
       "        0, -1, -1,  1, -1,  1,  0,  0, -1,  2,  0, -1, -1, -1,  0,  1,  0,\n",
       "       -1, -1, -1,  0,  0,  1, -1,  1, -1, -1, -1, -1, -1,  0, -1, -1,  0,\n",
       "       -1,  0, -1,  2, -1, -1,  0, -1, -1,  1, -1,  0, -1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp0-test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
