{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "from six.moves import xrange\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rev_3.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels_org = f.read()\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])\n",
    "reviews = all_text.split('\\n')\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "vocab_to_int['UNK']=0\n",
    "int_to_vocab=dict(zip(vocab_to_int.values(),vocab_to_int.keys()))\n",
    "#int_to_vocab[0]='_'\n",
    "reviews_ints = []\n",
    "data=[]\n",
    "for each in reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "for ee in words:\n",
    "    data.append(vocab_to_int[ee])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del counts\n",
    "del vocab\n",
    "del words\n",
    "del all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data [315, 1369, 5, 11, 395, 12, 20, 46, 396, 140] ['Which', 'boxerengined', 'BMW', 'is', 'right', 'for', 'you', 'We', 'test', 'them']\n",
      "3828\n",
      "UNK\n",
      "the\n",
      "Tesla\n",
      "a\n",
      "to\n",
      "BMW\n",
      "in\n",
      "cars\n",
      "and\n",
      "of\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Sample data', data[:10], [int_to_vocab[i] for i in data[:10]])\n",
    "data_index = 0\n",
    "print(max(int_to_vocab.keys()))\n",
    "for i in range(10):\n",
    "    print(int_to_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1369 boxerengined -> 315 Which\n",
      "1369 boxerengined -> 5 BMW\n",
      "5 BMW -> 11 is\n",
      "5 BMW -> 1369 boxerengined\n",
      "11 is -> 395 right\n",
      "11 is -> 5 BMW\n",
      "395 right -> 12 for\n",
      "395 right -> 11 is\n",
      "12 for -> 395 right\n",
      "12 for -> 20 you\n",
      "20 you -> 12 for\n",
      "20 you -> 46 We\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = int(2 * skip_window + 1)  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    #print('Data Index->',data_index)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0 \n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    #print('buffer->',buffer)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        #print('context_words->',context_words)\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        #print('words_to_use->',words_to_use)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            #buffer[0:3] = data[0:span]\n",
    "            buffer.extend(data[:span])\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=12, num_skips=2, skip_window=1)\n",
    "for i in range(12):\n",
    "    print(batch[i], int_to_vocab[batch[i]],\n",
    "        '->', labels[i, 0], int_to_vocab[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 30  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "vocabulary_size=int(max(int_to_vocab.keys()))+1# 1 for the UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalized\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Input data.\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "        # Look up embeddings for inputs.\n",
    "embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        # Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                    biases=nce_biases,\n",
    "                                    labels=train_labels,\n",
    "                                    inputs=embed,\n",
    "                                    num_sampled=num_sampled,\n",
    "                                    num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "#tf.summary.image('similarity',tf.reshape(similarity,[])\n",
    "tf.summary.image('normalized_embeddings',tf.reshape(normalized_embeddings,[1,vocabulary_size,embedding_size,1]))\n",
    "merged = tf.summary.merge_all()\n",
    "    # Add variable initializer.\n",
    "init = tf.global_variables_initializer()\n",
    "print('Finalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  178.670349121\n",
      "Nearest to me: best, 30, integrity, comes, Range, Denmark, CEO, Consider,\n",
      "Nearest to Series: credit, sustainable, P85, PHEV, build, iLove, scared, called,\n",
      "Nearest to want: c230, handbrake, S90, cheating, support, needed, cannot, blk,\n",
      "Nearest to up: tanker, Oxnard, With, Owner, exacerbate, most, Batteries, Bringing,\n",
      "Nearest to I: Cadillac, streets, MINI, Victorious, efforts, programmes, Ill, glow,\n",
      "Nearest to bmw: Dreams, iLove, well, ourâ€¦, legendary, gate, 37PM, God,\n",
      "Nearest to Check: 80000, responsible, so, 714, offer, tax, Alabama, managed,\n",
      "Nearest to from: Ill, LX, electriccar, future, never, Europe, R18, Mile,\n",
      "Nearest to Im: Martin, Allspace, hilariously, summer, stability, processors, case, Watch,\n",
      "Nearest to or: tells, Lowered, ups, Classic, boost, PD, Compass, silent,\n",
      "Nearest to by: 60, beast, Canada, Saves, M1, effects, disprove, Vehicle,\n",
      "Nearest to RT: BMW, Up, s37359680, By, TESLA, Premiere, button, 11500,\n",
      "Nearest to year: At, Annual, Engine, brand, civics, Navigation, pushed, Meet,\n",
      "Nearest to this: posted, Update, maintenance, furiously, Thefts, enjoy, becomes, Quartz,\n",
      "Nearest to Tesla: Day, low, dkjust, bringing, EXCELLENT, tht, rid, fresh,\n",
      "Nearest to Model: 15, make, cedar, Italy, Instrumented, Might, Foam, blk,\n",
      "Average loss at step  2000 :  19.4356288482\n",
      "Average loss at step  4000 :  4.26718124843\n",
      "Average loss at step  6000 :  3.80871192348\n",
      "Average loss at step  8000 :  3.48523546195\n",
      "Average loss at step  10000 :  3.2447302804\n",
      "Nearest to me: four, comes, Row, Denmark, Beetles, Winter, rolling, gave,\n",
      "Nearest to Series: credit, 32, Duluth, gonna, build, roads, recalled, newest,\n",
      "Nearest to want: get, see, support, Preowned, Hey, come, Prices, blk,\n",
      "Nearest to up: spain, Owner, tanker, Your, Batteries, upâ€™, S4, Oxnard,\n",
      "Nearest to I: Ill, Dodge, cent, Ah, ones, found, options, Chevy,\n",
      "Nearest to bmw: iLove, HIGHLINE, posted, programmed, Fact, M5, Greg, Greater,\n",
      "Nearest to Check: 80000, Economy, Scenic, responsible, 714, Tried, managed, replacing,\n",
      "Nearest to from: theyre, CrossWithout, Hillclimb, roof, HTD, whenever, Challengers, Tomorrow,\n",
      "Nearest to Im: about, stability, Allspace, hilariously, ya, afford, About, bmwreleasesvideosonthehistoryofthe5series,\n",
      "Nearest to or: tells, R32, Orientation, Island, boost, am, Condition, puke,\n",
      "Nearest to by: Panasonic, Automoblog, emilymorenzoni, All, Shareholders, 60, VIDEO, Sellers,\n",
      "Nearest to RT: s37359680, Wang, Premiere, TESLA, language, anyway, Bringing, license,\n",
      "Nearest to year: brand, 160, Meet, Dark, Machine, solar, help, civics,\n",
      "Nearest to this: Global, pronounced, smitten, talk, Tomorrow, OfferUp, Come, Reversed,\n",
      "Nearest to Tesla: tht, Blanka, Wings, mammograms, Id, Ridgelines, retro, TITAN,\n",
      "Nearest to Model: 15, Foam, Instrumented, Gets, M1, exists, Trail, connectedDrive,\n",
      "Average loss at step  12000 :  3.03101777625\n",
      "Average loss at step  14000 :  2.85539123428\n",
      "Average loss at step  16000 :  2.70745504063\n",
      "Average loss at step  18000 :  2.57975991976\n",
      "Average loss at step  20000 :  2.46534703273\n",
      "Nearest to me: drinking, feel, gave, rolling, Implications, Energy, care, anyhow,\n",
      "Nearest to Series: Show, roads, where, Debut, recalled, remind, US, Skype,\n",
      "Nearest to want: Preowned, come, wanna, believe, picture, Hey, see, avoid,\n",
      "Nearest to up: spain, CG, S4, aesthetics, Hype, slippy, Earn, kms,\n",
      "Nearest to I: wealthy, we, options, Ill, Holy, Heavens, Dodge, Part,\n",
      "Nearest to bmw: HIGHLINE, iLove, programmed, belts, Save, M5, Greater, flashy,\n",
      "Nearest to Check: Economy, Scenic, responsible, 80000, mfg, door, Id, Oooooh,\n",
      "Nearest to from: theyre, CrossWithout, HTD, Hillclimb, Island, United, roof, Challengers,\n",
      "Nearest to Im: about, stability, Allspace, guys, Inside, Mickey, ya, hilariously,\n",
      "Nearest to or: R32, am, Island, Condition, Orientation, tells, boost, C,\n",
      "Nearest to by: SUR, Shareholders, Automoblog, Panasonic, press, Rallye, Sellers, Heated,\n",
      "Nearest to RT: Wang, Premiere, Accessories, anyway, tornado, GLE63, license, TESLA,\n",
      "Nearest to year: brand, Machine, time, solar, Dark, civics, week, northeast,\n",
      "Nearest to this: pronounced, Global, greatness, Texting, profit, astronauts, Tomorrow, OfferUp,\n",
      "Nearest to Tesla: mammograms, Wings, Blanka, retro, Inc, tht, himself, Ridgelines,\n",
      "Nearest to Model: 15, P90D, Might, Instrumented, GeNeRaTioN, M1, connectedDrive, Foam,\n",
      "Average loss at step  22000 :  2.3757520963\n",
      "Average loss at step  24000 :  2.30237128931\n",
      "Average loss at step  26000 :  2.23422250295\n",
      "Average loss at step  28000 :  2.1764256382\n",
      "Average loss at step  30000 :  2.1331586659\n",
      "Nearest to me: feel, gave, drinking, care, maybe, Energy, brands, tested,\n",
      "Nearest to Series: Show, roads, Debut, US, Skype, software, Revealing, where,\n",
      "Nearest to want: come, Preowned, picture, believe, wanna, particular, avoid, fairly,\n",
      "Nearest to up: S4, Earn, aesthetics, CG, spain, Hype, fixedI, slippy,\n",
      "Nearest to I: wealthy, options, we, Part, plate, Heavens, Ill, R1100,\n",
      "Nearest to bmw: HIGHLINE, built, iLove, belts, developing, department, parts, share,\n",
      "Nearest to Check: Economy, Scenic, mfg, responsible, door, Amid, semester, Oooooh,\n",
      "Nearest to from: theyre, CrossWithout, HTD, Island, United, allelectric, Mazda, jealous,\n",
      "Nearest to Im: about, stability, ya, Inside, bmwreleasesvideosonthehistoryofthe5series, Mickey, Turmoil, Allspace,\n",
      "Nearest to or: R32, Island, am, Condition, C, boost, Orientation, radiator,\n",
      "Nearest to by: SUR, Shareholders, ST, Automoblog, HT, Wedges, press, Heated,\n",
      "Nearest to RT: Accessories, Wang, Premiere, anyway, GLE63, Member, 5, Mazda3,\n",
      "Nearest to year: time, Dark, Machine, solar, civics, Fob, week, lovers,\n",
      "Nearest to this: pronounced, profit, Texting, greatness, Global, bag, Predict, Silodromecom,\n",
      "Nearest to Tesla: mammograms, Wings, Blanka, person, Ridgelines, retro, AND, everything,\n",
      "Nearest to Model: P90D, GeNeRaTioN, Might, figures, lawsuit, 15, Instrumented, connectedDrive,\n",
      "Average loss at step  32000 :  2.10098642874\n",
      "Average loss at step  34000 :  2.06919498575\n",
      "Average loss at step  36000 :  2.04173817694\n",
      "Average loss at step  38000 :  2.03214885998\n",
      "Average loss at step  40000 :  2.00487492579\n",
      "Nearest to me: gave, brands, maybe, tested, drinking, feel, care, Energy,\n",
      "Nearest to Series: Show, US, Skype, software, roads, Debut, Revealing, SERIES,\n",
      "Nearest to want: come, Preowned, picture, particular, about, avoid, wanna, believe,\n",
      "Nearest to up: S4, Earn, Stretched, CG, slippy, ratings, aesthetics, fixedI,\n",
      "Nearest to I: options, wealthy, we, Part, still, Ill, R1100, plate,\n",
      "Nearest to bmw: HIGHLINE, built, department, belts, share, developing, wearing, solid,\n",
      "Nearest to Check: Economy, Scenic, mfg, Amid, responsible, door, semester, baller,\n",
      "Nearest to from: theyre, Island, CrossWithout, HTD, allelectric, Mazda, Coilover, Convex,\n",
      "Nearest to Im: about, bmwreleasesvideosonthehistoryofthe5series, ya, parking, Motorâ€¦, belts, stability, Mickey,\n",
      "Nearest to or: R32, Island, C, am, defense, Condition, boost, remote,\n",
      "Nearest to by: SUR, ST, HT, Shareholders, Wedges, send, Rallye, Automoblog,\n",
      "Nearest to RT: Accessories, Premiere, anyway, Wang, GLE63, Tires, Member, aims,\n",
      "Nearest to year: time, Fob, Dark, civics, solar, bloody, Machine, week,\n",
      "Nearest to this: profit, pronounced, greatness, Texting, lasted, bag, Silodromecom, Global,\n",
      "Nearest to Tesla: mammograms, Blanka, everything, Wings, person, Ridgelines, variant, pedal,\n",
      "Nearest to Model: P90D, figures, lawsuit, public, eand, design, grandma, Might,\n",
      "Average loss at step  42000 :  1.98663723391\n",
      "Average loss at step  44000 :  1.97699445832\n",
      "Average loss at step  46000 :  1.96100232244\n",
      "Average loss at step  48000 :  1.9526620692\n",
      "Average loss at step  50000 :  1.94147576261\n",
      "Nearest to me: gave, tested, maybe, brands, drinking, feel, care, Engineer,\n",
      "Nearest to Series: Show, US, software, Skype, Debut, alleged, roads, Eus,\n",
      "Nearest to want: come, about, picture, Preowned, particular, avoid, when, fairly,\n",
      "Nearest to up: S4, Earn, Stretched, CG, Hype, slippy, ratings, aesthetics,\n",
      "Nearest to I: wealthy, options, we, Part, R1100, checks, dont, Ill,\n",
      "Nearest to bmw: HIGHLINE, built, department, solid, thanks, belts, wearing, autonomouscar,\n",
      "Nearest to Check: Economy, Scenic, Amid, mfg, responsible, semester, door, baller,\n",
      "Nearest to from: theyre, Island, CrossWithout, Coilover, allelectric, HTD, Mazda, Convex,\n",
      "Nearest to Im: bmwreleasesvideosonthehistoryofthe5series, ya, about, Motorâ€¦, Allspace, parking, belts, Mickey,\n",
      "Nearest to or: R32, Island, C, am, defense, boost, core, may,\n",
      "Nearest to by: ST, SUR, HT, Wedges, Shareholders, send, Automoblog, cells,\n",
      "Nearest to RT: Accessories, GLE63, anyway, Premiere, aims, Wang, addition, Utility,\n",
      "Nearest to year: time, Dark, Fob, bloody, solar, civics, humiliates, week,\n",
      "Nearest to this: profit, pronounced, greatness, Texting, Silodromecom, a, lasted, Murano,\n",
      "Nearest to Tesla: mammograms, Blanka, everything, person, Wings, Ridgelines, retro, variant,\n",
      "Nearest to Model: figures, public, eand, grandma, lawsuit, P90D, Niro, BMWToyota,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  1.93395145905\n",
      "Average loss at step  54000 :  1.9274724384\n",
      "Average loss at step  56000 :  1.91975568908\n",
      "Average loss at step  58000 :  1.91844945377\n",
      "Average loss at step  60000 :  1.90813187644\n",
      "Nearest to me: gave, tested, brands, maybe, drinking, GeNeRaTioN, feel, realized,\n",
      "Nearest to Series: Show, US, Eus, software, Debut, alleged, Revealing, Skype,\n",
      "Nearest to want: come, about, picture, Preowned, particular, avoid, when, sat,\n",
      "Nearest to up: S4, Earn, Stretched, CG, ratings, Hills, reported, Hype,\n",
      "Nearest to I: options, we, wealthy, Part, R1100, checks, Ill, seriously,\n",
      "Nearest to bmw: HIGHLINE, built, department, solid, belts, wearing, thanks, parts,\n",
      "Nearest to Check: Economy, Scenic, Amid, mfg, responsible, semester, baller, door,\n",
      "Nearest to from: theyre, Island, Coilover, allelectric, CrossWithout, Mazda, HTD, Kawasaki,\n",
      "Nearest to Im: bmwreleasesvideosonthehistoryofthe5series, ya, Motorâ€¦, about, Allspace, communications, belts, parking,\n",
      "Nearest to or: R32, Island, defense, C, am, core, boost, hausofminiscom,\n",
      "Nearest to by: HT, ST, SUR, Wedges, send, cells, noise, Shareholders,\n",
      "Nearest to RT: Accessories, GLE63, anyway, aims, Premiere, Wang, Cafe, Safe,\n",
      "Nearest to year: time, Fob, Dark, bloody, complete, humiliates, solar, Serious,\n",
      "Nearest to this: greatness, profit, pronounced, Texting, Silodromecom, a, lasted, OfferUp,\n",
      "Nearest to Tesla: mammograms, Blanka, everything, Wings, variant, Ridgelines, person, AND,\n",
      "Nearest to Model: figures, eand, Niro, public, lawsuit, grandma, BMWToyota, bet,\n",
      "Average loss at step  62000 :  1.89863473409\n",
      "Average loss at step  64000 :  1.89635459065\n",
      "Average loss at step  66000 :  1.89633375704\n",
      "Average loss at step  68000 :  1.8916546531\n",
      "Average loss at step  70000 :  1.88617676184\n",
      "Nearest to me: gave, tested, brands, maybe, drinking, realized, China, GeNeRaTioN,\n",
      "Nearest to Series: Show, US, Eus, alleged, SERIES, Hyundai, software, Debut,\n",
      "Nearest to want: come, about, particular, picture, Preowned, sat, when, avoid,\n",
      "Nearest to up: S4, Earn, Stretched, Hills, carmaker, CG, Hype, reported,\n",
      "Nearest to I: we, R1100, dont, options, seriously, m5, wealthy, Part,\n",
      "Nearest to bmw: HIGHLINE, department, built, solid, belts, wearing, thanks, autonomouscar,\n",
      "Nearest to Check: Economy, Scenic, Amid, mfg, semester, responsible, baller, Cops,\n",
      "Nearest to from: theyre, Island, allelectric, Coilover, CrossWithout, Mazda, Kawasaki, Impressive,\n",
      "Nearest to Im: bmwreleasesvideosonthehistoryofthe5series, ya, about, Motorâ€¦, Allspace, Mickey, communications, Duluth,\n",
      "Nearest to or: defense, R32, Island, C, core, boost, hausofminiscom, am,\n",
      "Nearest to by: HT, ST, SUR, cells, send, noise, Italian, Wedges,\n",
      "Nearest to RT: aims, GLE63, Accessories, anyway, Premiere, Safe, Wang, addition,\n",
      "Nearest to year: time, Dark, Fob, bloody, complete, week, humiliates, Serious,\n",
      "Nearest to this: greatness, profit, pronounced, a, Silodromecom, Interesting, lasted, Texting,\n",
      "Nearest to Tesla: Blanka, mammograms, variant, everything, Wings, Ridgelines, master, retro,\n",
      "Nearest to Model: eand, grandma, figures, bet, Niro, public, design, BMWToyota,\n",
      "Average loss at step  72000 :  1.88869149667\n",
      "Average loss at step  74000 :  1.87985589892\n",
      "Average loss at step  76000 :  1.87380615789\n",
      "Average loss at step  78000 :  1.87023471805\n",
      "Average loss at step  80000 :  1.87261834627\n",
      "Nearest to me: gave, tested, maybe, brands, drinking, GeNeRaTioN, fairly, realized,\n",
      "Nearest to Series: Show, Eus, US, alleged, newest, SERIES, Activity, Hyundai,\n",
      "Nearest to want: come, about, particular, picture, when, sat, Preowned, avoid,\n",
      "Nearest to up: S4, Earn, Stretched, Hills, carmaker, reported, Hype, ratings,\n",
      "Nearest to I: dont, seriously, R1100, we, checks, m5, Ill, type,\n",
      "Nearest to bmw: HIGHLINE, department, built, solid, thanks, wearing, belts, getting,\n",
      "Nearest to Check: Economy, Scenic, Amid, semester, mfg, responsible, baller, Cops,\n",
      "Nearest to from: theyre, Island, allelectric, Coilover, Impressive, delay, Mazda, Kawasaki,\n",
      "Nearest to Im: bmwreleasesvideosonthehistoryofthe5series, ya, Motorâ€¦, Duluth, about, Allspace, honda, communications,\n",
      "Nearest to or: defense, Island, R32, C, boost, hausofminiscom, core, am,\n",
      "Nearest to by: HT, ST, SUR, cells, Italian, send, noise, hydrogen,\n",
      "Nearest to RT: GLE63, aims, anyway, Accessories, Safe, Premiere, addition, ELECTRIC,\n",
      "Nearest to year: time, complete, bloody, Dark, Fob, humiliates, Serious, week,\n",
      "Nearest to this: profit, greatness, pronounced, a, lasted, Silodromecom, Texting, OfferUp,\n",
      "Nearest to Tesla: Blanka, variant, mammograms, master, Guests, crutches, Wings, mustang,\n",
      "Nearest to Model: eand, figures, bet, Niro, grandma, public, lawsuit, BMWToyota,\n",
      "Average loss at step  82000 :  1.87196270934\n",
      "Average loss at step  84000 :  1.86537874812\n",
      "Average loss at step  86000 :  1.8661521818\n",
      "Average loss at step  88000 :  1.86510841697\n",
      "Average loss at step  90000 :  1.85964210963\n",
      "Nearest to me: gave, tested, maybe, brands, realized, drinking, China, GeNeRaTioN,\n",
      "Nearest to Series: Show, Eus, US, newest, stealing, Activity, Hyundai, alleged,\n",
      "Nearest to want: come, about, particular, sat, picture, when, avoid, Preowned,\n",
      "Nearest to up: S4, Earn, Stretched, carmaker, Hills, reported, puts, anything,\n",
      "Nearest to I: R1100, dont, checks, type, Ill, we, m5, Part,\n",
      "Nearest to bmw: HIGHLINE, department, built, solid, thanks, wearing, getting, belts,\n",
      "Nearest to Check: Economy, Amid, Scenic, semester, mfg, baller, responsible, hanging,\n",
      "Nearest to from: Island, theyre, allelectric, Impressive, Mazda, Coilover, delay, CrossWithout,\n",
      "Nearest to Im: bmwreleasesvideosonthehistoryofthe5series, ya, Duluth, Allspace, Motorâ€¦, about, Scott, Though,\n",
      "Nearest to or: defense, Island, R32, C, hausofminiscom, boost, this, core,\n",
      "Nearest to by: HT, ST, SUR, cells, Italian, send, hydrogen, someone,\n",
      "Nearest to RT: GLE63, aims, anyway, Safe, Accessories, Premiere, AU, Utility,\n",
      "Nearest to year: time, complete, bloody, Dark, Fob, week, clean, Serious,\n",
      "Nearest to this: greatness, profit, pronounced, a, Silodromecom, OfferUp, Interesting, lasted,\n",
      "Nearest to Tesla: variant, Blanka, mammograms, Ridgelines, Guests, So, crutches, AND,\n",
      "Nearest to Model: eand, bet, grandma, figures, Niro, public, lawsuit, BMWToyota,\n",
      "Average loss at step  92000 :  1.85783814642\n",
      "Average loss at step  94000 :  1.8619584139\n",
      "Average loss at step  96000 :  1.85805996221\n",
      "Average loss at step  98000 :  1.84676805764\n",
      "Average loss at step  100000 :  1.85307872248\n",
      "Nearest to me: gave, tested, maybe, realized, brands, China, feel, fairly,\n",
      "Nearest to Series: Show, Eus, newest, stealing, US, Activity, alleged, generating,\n",
      "Nearest to want: come, about, particular, sat, when, picture, avoid, instead,\n",
      "Nearest to up: S4, Earn, Stretched, Hills, carmaker, reported, anything, puts,\n",
      "Nearest to I: R1100, dont, checks, seriously, type, we, Part, Ill,\n",
      "Nearest to bmw: HIGHLINE, department, built, getting, solid, belts, thanks, wearing,\n",
      "Nearest to Check: Economy, semester, Amid, Scenic, mfg, baller, hanging, responsible,\n",
      "Nearest to from: allelectric, delay, Island, Impressive, theyre, fourcylinders, Mazda, earlier,\n",
      "Nearest to Im: bmwreleasesvideosonthehistoryofthe5series, ya, Duluth, Motorâ€¦, about, Scott, Allspace, Though,\n",
      "Nearest to or: defense, R32, Island, C, hausofminiscom, boost, core, this,\n",
      "Nearest to by: HT, ST, SUR, cells, Italian, hydrogen, makers, send,\n",
      "Nearest to RT: GLE63, aims, anyway, Safe, Accessories, Premiere, Confirms, Cafe,\n",
      "Nearest to year: time, complete, Dark, bloody, Fob, week, humiliates, clean,\n",
      "Nearest to this: greatness, profit, pronounced, a, Silodromecom, lasted, Interesting, OfferUp,\n",
      "Nearest to Tesla: variant, Blanka, Guests, Sources, mustang, infrastructure, Replacement, master,\n",
      "Nearest to Model: eand, bet, figures, grandma, Niro, lawsuit, public, P90D,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps = 100001\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    train_writer = tf.summary.FileWriter('train_II', sess.graph)\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "         # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "        average_loss += loss_val\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                summary=merged.eval(feed_dict=feed_dict)\n",
    "                train_writer.add_summary(summary, step)\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            \n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = int_to_vocab[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = int_to_vocab[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    saver.save(sess, \"checkpoints/separate.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "saver = tf.train.Saver()\n",
    "num_steps=100\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"checkpoints/separate.ckpt\")\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "         # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        if step % 2 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = int_to_vocab[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = int_to_vocab[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 30\n",
      "1000 [315, 1369, 5, 11, 395, 12, 20, 46, 396, 140, 65, 1370, 4, 397, 24]\n",
      "Zero-length reviews: 0\n",
      "Maximum review length: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  315, 1369,    5,   11,  395,   12,   20,\n",
       "          46,  396,  140,   65, 1370,    4,  397,   24],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    5, 1371,  778,  316,   82,  213, 1372],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    5, 1373,\n",
       "         398, 1374,   92,  779, 1375,    4,  317,    6],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  189,  214,  215, 1376,  248, 1377],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    5,  249,  190,  399,    5,  249,  190],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "        1378,   58,    3,    5,  780,   11,    3, 1379,  534, 1380, 1381,\n",
       "         318, 1382, 1383,   13, 1384,   11, 1385,  535],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    5,  191,  781,\n",
       "           3, 1386,   31, 1387,  782,   22,  536,  216],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    5,  191,    4,  537,\n",
       "           3,  536,  216,   22,    3,  538,    9, 1388],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "        1389, 1390,  113,   55,    5,  783, 1391,   93],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    5,   53, 1392,\n",
       "         784,  165,    8,  166,  539,  785,  540,  141]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[1]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "reviews_ints = [r[0:100] for r in reviews_ints if len(r) > 0]\n",
    "print(len(reviews_ints),reviews_ints[0])\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "\n",
    "seq_len = 30\n",
    "features = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "for i, row in enumerate(reviews_ints):\n",
    "    features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "features[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#np.save('towtiw',final_embeddings)\n",
    "#final_embeddings=np.load('towtiw.npy')\n",
    "R=np.zeros((embedding_size,seq_len))\n",
    "m=len(features)\n",
    "def write_csv(data):\n",
    "    with open('Seq_Embo_f.csv', 'a') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(data)\n",
    "for k in xrange(m):\n",
    "    for i in xrange(seq_len):\n",
    "        R[:,i]=final_embeddings[features[k][i]]\n",
    "    write_csv(R.reshape(-1))\n",
    "    if k%10==0:\n",
    "        clear_output()\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFH6RSCn8IpFS+EUi9T/uZ89aHg9wwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8408060320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.read_csv('Seq_Embo_f.csv',header=None)\n",
    "A=np.array(df.iloc[4])\n",
    "A=A.reshape(-1,seq_len)\n",
    "plt.imshow(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "[1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1 if l == \"positive\" else 0 for l in labels_org.split()])\n",
    "print(len(labels))\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb=pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb.to_csv('labels.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  1\n",
       "3  0\n",
       "4  1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll=pd.read_csv('labels.csv',header=None)\n",
    "ll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array(ll[:][:]).reshape(-1)\n",
    "max(np.abs(A[:100]-np.array(labels[:1000]).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74073, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
